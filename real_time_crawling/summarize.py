# summarize.py
import sys
import os
import json
import re
from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast

SAVE_DIR = "/Users/gihyeon/Downloads/news_crawling/"
MODEL_NAME = "gogamza/kobart-summarization"

def load_articles(file_path):
    with open(file_path, 'r', encoding='utf8') as f:
        return json.load(f)

def save_articles(file_path, articles):
    with open(file_path, 'w', encoding='utf8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=4)

def summarize_text(text, model, tokenizer):
    input_ids = tokenizer.encode(text, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = model.generate(input_ids, max_length=100, min_length=20, length_penalty=2.0, num_beams=4, early_stopping=True)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

def main():
    if len(sys.argv) < 2:
        print("âŒ ê²€ìƒ‰ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    query = sys.argv[1]
    safe_query = re.sub(r"[^\w\sê°€-í£-]", "", query).strip()
    file_path = os.path.join(SAVE_DIR, f"{safe_query}_naver_news.json")
    if not os.path.exists(file_path):
        print("âŒ JSON íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤:", file_path)
        return

    print("âœ… ëª¨ë¸ ë¡œë”© ì¤‘...")
    tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_NAME)
    model = BartForConditionalGeneration.from_pretrained(MODEL_NAME)

    articles = load_articles(file_path)

    for article in articles:
        text = re.sub('<[^>]+>', '', article.get("description", ""))
        try:
            summary = summarize_text(text, model, tokenizer)
            article["summary"] = summary
            print(f"ğŸ§  ìš”ì•½ ì™„ë£Œ â†’ {summary}")
        except Exception as e:
            print(f"âŒ ìš”ì•½ ì‹¤íŒ¨ â†’ {e}")
            article["summary"] = "(ìš”ì•½ ì‹¤íŒ¨)"

    save_articles(file_path, articles)
    print(f"âœ… ìš”ì•½ ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {file_path}")

if __name__ == "__main__":
    main()