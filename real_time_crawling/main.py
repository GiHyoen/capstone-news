from fastapi import FastAPI, Request
from pydantic import BaseModel
from typing import List, Optional
from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast
import re

app = FastAPI()

# âœ… ëª¨ë¸ ë¡œë“œ
MODEL_NAME = "gogamza/kobart-summarization"
tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_NAME)
model = BartForConditionalGeneration.from_pretrained(MODEL_NAME)

# âœ… ëª¨ë¸ ì…ë ¥ íƒ€ì… ì •ì˜
class Article(BaseModel):
    cnt: int
    title: str
    description: str
    org_link: str
    link: str
    pDate: str
    image: Optional[str] = None

class SummaryRequest(BaseModel):
    articles: List[Article]

# âœ… HTML/íŠ¹ìˆ˜ë¬¸ì ì •ì œ í•¨ìˆ˜
def clean_html(text: str) -> str:
    text = re.sub(r"<[^>]+>", "", text)  # íƒœê·¸ ì œê±°
    text = re.sub(r"&[a-z]+;", "", text)  # HTML ì—”í‹°í‹° ì œê±°
    text = re.sub(r"[<>]+", "", text)     # íŠ¹ìˆ˜ ê´„í˜¸ ì œê±°
    text = text.replace("â€˜", "").replace("â€™", "")
    return text.strip()

# âœ… ì…ë ¥ í…ìŠ¤íŠ¸ êµ¬ì„± (ì¤‘ë³µ ì œëª© ì œê±° í¬í•¨)
def prepare_input(title: str, description: str) -> str:
    title = clean_html(title)
    description = clean_html(description)
    if title in description:
        return description
    else:
        return f"{title} {description}"

# âœ… ìš”ì•½ í›„ì²˜ë¦¬ í•¨ìˆ˜ (ì¤‘ë³µ ë¬¸ì¥/ë‹¨ì–´ ì œê±°, ì•ë¶€ë¶„ë§Œ ìœ ì§€)
def clean_summary(text: str) -> str:
    text = text.strip().replace("\n", " ")
    text = re.sub(r"&[a-z]+;", "", text)  # HTML entity ì œê±°
    text = re.sub(r"(.)\1{3,}", r"\1", text)  # ê¸€ì ë°˜ë³µ ì œê±°
    text = re.sub(r"(\b\w+\b)( \1\b)+", r"\1", text)  # ë‹¨ì–´ ë°˜ë³µ ì œê±°

    sentences = re.split(r"(?<=[.!?])\s+", text)
    seen = set()
    cleaned = []
    for s in sentences:
        s = s.strip()
        if s and s not in seen and len(s) > 5:
            seen.add(s)
            cleaned.append(s)
    return " ".join(cleaned[:3])

# âœ… FastAPI ìš”ì•½ ì—”ë“œí¬ì¸íŠ¸
@app.post("/api/summarize")
async def summarize_articles(request: Request):
    try:
        body = await request.json()
        request_data = SummaryRequest(**body)
    except Exception as e:
        return {"error": str(e)}

    summarized = []

    for article in request_data.articles:
        input_text = prepare_input(article.title, article.description)
        input_ids = tokenizer.encode(
            input_text,
            return_tensors="pt",
            max_length=512,
            truncation=True
        )

        summary_ids = model.generate(
            input_ids,
            max_length=100,
            min_length=20,
            num_beams=4,
            no_repeat_ngram_size=3,     # ğŸ”‘ ë°˜ë³µ ë°©ì§€ í•µì‹¬
            repetition_penalty=2.5,
            length_penalty=1.0,
            early_stopping=True
        )

        raw_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        final_summary = clean_summary(raw_summary)

        summarized.append({
            "title": article.title,
            "summary": final_summary,
            "link": article.link,
            "date": article.pDate,
            "image": article.image
        })

    return {"results": summarized}